name: Run FPL Engine

on:
  workflow_dispatch:
  schedule:
    - cron: "0 8 * * FRI"

jobs:
  run-engine:
    runs-on: ubuntu-latest

    steps:
      # -------------------------------------------------------
      # 1. Checkout repo
      # -------------------------------------------------------
      - name: Checkout repository
        uses: actions/checkout@v4

      # -------------------------------------------------------
      # 2. Set up Python
      # -------------------------------------------------------
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # -------------------------------------------------------
      # 3. Install dependencies
      # -------------------------------------------------------
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # -------------------------------------------------------
      # 4. Fetch and parse FBref xG/xA (robust HTML → CSV)
      # -------------------------------------------------------
      - name: Download FBref HTML
        run: |
          mkdir -p data
          curl -L "https://fbref.com/en/comps/9/Premier-League-Stats" \
            -H "User-Agent: Mozilla/5.0" \
            -o data/fbref.html

    - name: Extract xG/xA from FBref HTML (BeautifulSoup)
        run: |
          python -m pip install --quiet beautifulsoup4 lxml
          python - << 'EOF'
          import pandas as pd
          import numpy as np
          from bs4 import BeautifulSoup, Comment

          # Map FBref team names -> FPL team_short
          TEAM_MAP = {
              "Arsenal": "ARS",
              "Aston Villa": "AVL",
              "Bournemouth": "BOU",
              "Brentford": "BRE",
              "Brighton": "BHA",
              "Chelsea": "CHE",
              "Crystal Palace": "CRY",
              "Everton": "EVE",
              "Fulham": "FUL",
              "Liverpool": "LIV",
              "Luton Town": "LUT",
              "Manchester City": "MCI",
              "Manchester Utd": "MUN",
              "Newcastle Utd": "NEW",
              "Nottingham Forest": "NFO",
              "Sheffield Utd": "SHU",
              "Tottenham": "TOT",
              "West Ham": "WHU",
              "Wolverhampton Wanderers": "WOL",
          }

          def parse_number(s):
              s = (s or "").strip()
              if s in ("", "-", "NA"):
                  return 0.0
              try:
                  return float(s)
              except Exception:
                  return 0.0

          try:
              # Read saved HTML
              with open("data/fbref.html", encoding="utf-8") as f:
                  html = f.read()

              soup = BeautifulSoup(html, "lxml")

              # FBref often hides tables in HTML comments, so parse both normal and commented tables
              tables = []

              # Visible tables
              tables.extend(soup.find_all("table"))

              # Tables inside comments
              for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
                  if "<table" in comment:
                      frag = BeautifulSoup(comment, "lxml")
                      tables.extend(frag.find_all("table"))

              print(f"[DEBUG] Found {len(tables)} total <table> elements (visible + commented).")

              target_table = None

              # Heuristic: find table with expected stats: data-stat fields xg/xa/npxg etc.
              for idx, table in enumerate(tables):
                  header_th = table.find("thead")
                  if not header_th:
                      continue
                  header_cells = [th.get_text(strip=True).lower() for th in header_th.find_all("th")]

                  has_player = any("player" in h for h in header_cells)
                  has_squad = any("squad" in h for h in header_cells)
                  has_xg = any("xg" in h for h in header_cells)
                  has_xa = any("xa" in h for h in header_cells)

                  if has_player and has_squad and has_xg and has_xa:
                      print(f"[DEBUG] Candidate expected stats table at index {idx} with columns: {header_cells}")
                      target_table = table
                      break

              if target_table is None:
                  raise Exception("Could not find an expected stats table with player/squad/xG/xA columns.")

              rows_data = []
              tbody = target_table.find("tbody")
              if not tbody:
                  raise Exception("Target table has no <tbody>")

              for row in tbody.find_all("tr"):
                  if "class" in row.attrs and "thead" in row["class"]:
                      # Skip grouping header rows
                      continue

                  # Extract by data-stat attributes where possible
                  def get_stat(stat_name):
                      cell = row.find("td", {"data-stat": stat_name})
                      if cell is None:
                          return ""
                      return cell.get_text(strip=True)

                  player = get_stat("player")
                  squad = get_stat("squad")
                  xg = get_stat("xg")
                  xa = get_stat("xa")
                  npxg = get_stat("npxg")
                  npxg_xa = get_stat("npxg_xa")
                  minutes = get_stat("minutes") or get_stat("min") or get_stat("90s")

                  if not player or not squad:
                      continue

                  # Convert minutes from 90s to real minutes if needed
                  if get_stat("90s") and not get_stat("minutes") and not get_stat("min"):
                      minutes_val = parse_number(minutes) * 90.0
                  else:
                      minutes_val = parse_number(minutes)

                  team_short = TEAM_MAP.get(squad, squad)

                  rows_data.append(
                      {
                          "web_name": player,
                          "team_short": team_short,
                          "xg": parse_number(xg),
                          "xa": parse_number(xa),
                          "npxg": parse_number(npxg),
                          "npxg_xa": parse_number(npxg_xa),
                          "minutes": minutes_val,
                      }
                  )

              if not rows_data:
                  raise Exception("Parsed 0 player rows from expected stats table.")

              out = pd.DataFrame(rows_data)
              out.to_csv("xg_xa_latest.csv", index=False)
              print(f"[SUCCESS] Created xg_xa_latest.csv with {len(out)} player rows.")

          except Exception as e:
              print("[ERROR] FBref xG/xA BeautifulSoup extraction failed:", e)
              print("[DEBUG] Writing empty fallback CSV.")
              pd.DataFrame(
                  {"web_name": [], "team_short": [], "xg": [], "xa": [], "npxg": [], "npxg_xa": [], "minutes": []}
              ).to_csv("xg_xa_latest.csv", index=False)
          EOF

      # -------------------------------------------------------
      # 5. Run engine
      # -------------------------------------------------------
      - name: Run FPL engine
        run: python engine_gw13.py

      # -------------------------------------------------------
      # 6. Generate plain-text summary for email (pure Python)
      # -------------------------------------------------------
      - name: Build email summary
        id: summary
        run: |
          python - << 'EOF'
          import glob
          import os
          from pathlib import Path

          import pandas as pd

          lines = ["FPL Engine Summary", ""]

          # Captain suggestion
          myteam_files = glob.glob("gw*_predictions_myteam.csv")
          if myteam_files:
              df = pd.read_csv(myteam_files[0])
              if "multi_gw_points" in df.columns:
                  df = df.sort_values("multi_gw_points", ascending=False)
                  top = df.iloc[0]
                  lines.append(
                      f"Suggested Captain: {top.web_name} ({top.team_short}) — {top.multi_gw_points:.2f} pts"
                  )
                  lines.append("")
              else:
                  lines.append("Captain suggestion unavailable (multi_gw_points missing).")
                  lines.append("")

          # Single-transfer suggestion
          single_files = glob.glob("gw*_transfer_suggestions.csv")
          if single_files:
              df = pd.read_csv(single_files[0])
              if not df.empty:
                  top = df.iloc[0]
                  lines.append("Top Single Transfer:")
                  lines.append(
                      f"Sell {top.sell_name} → Buy {top.buy_name} (Gain: {top.gain:.2f})"
                  )
                  lines.append("")

          # Double-transfer suggestion
          double_files = glob.glob("gw*_double_transfers.csv")
          if double_files:
              df = pd.read_csv(double_files[0])
              if not df.empty:
                  top = df.iloc[0]
                  lines.append("Top Double Transfer:")
                  lines.append(
                      f"Sell {top.sell1} & {top.sell2} → Buy {top.buy1} & {top.buy2} (Net Gain: {top.net_gain:.2f})"
                  )
                  lines.append("")

          summary_text = "\n".join(lines)

          # Write to file for debugging
          Path("summary.txt").write_text(summary_text, encoding="utf-8")

          # Expose via GITHUB_OUTPUT
          gh_out = os.environ.get("GITHUB_OUTPUT")
          if gh_out:
              with open(gh_out, "a", encoding="utf-8") as f:
                  print("summary_out<<EOF", file=f)
                  f.write(summary_text + "\n")
                  print("EOF", file=f)

          print(summary_text)
          EOF

      # -------------------------------------------------------
      # 7. ZIP outputs
      # -------------------------------------------------------
      - name: Zip outputs
        run: |
          mkdir -p artifacts
          zip artifacts/fpl-engine-csvs.zip *.csv

      # -------------------------------------------------------
      # 8. Upload artifacts
      # -------------------------------------------------------
      - name: Upload CSV artifacts
        uses: actions/upload-artifact@v4
        with:
          name: fpl-engine-csvs
          path: artifacts/fpl-engine-csvs.zip

      # -------------------------------------------------------
      # 9. Email results
      # -------------------------------------------------------
      - name: Email results
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          secure: starttls
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          from: ${{ secrets.EMAIL_FROM }}
          to: ${{ secrets.EMAIL_TO }}
          subject: "FPL Engine Results (Run #${{ github.run_number }})"
          body: |
            Your FPL engine run has completed.

            =============================
            FPL SUMMARY
            =============================

            ${{ steps.summary.outputs.summary_out }}

            -----------------------------
            Full run details:
            ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

            CSV outputs are attached.
          attachments: artifacts/fpl-engine-csvs.zip
